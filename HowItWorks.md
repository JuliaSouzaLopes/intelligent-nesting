# ğŸ¨ Como Funciona: Sistema de Nesting Inteligente

ExplicaÃ§Ã£o visual e intuitiva do funcionamento completo

---

## ğŸ¯ Problema: Nesting 2D

**Objetivo:** Colocar peÃ§as irregulares em uma chapa minimizando desperdÃ­cio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHAPA (Container)                  â”‚
â”‚  1000mm Ã— 600mm                     â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”  â•±â•²     â¬¡                   â”‚
â”‚  â”‚  â”‚ â•±  â•²   â¬¡ â¬¡                  â”‚
â”‚  â””â”€â”€â”˜ â”€â”€â”€â”€   â¬¡ â¬¡                  â”‚
â”‚                                     â”‚
â”‚        â† PeÃ§as para colocar         â”‚
â”‚                                     â”‚
â”‚  ğŸ’° Objetivo: Maximizar utilizaÃ§Ã£o  â”‚
â”‚     UtilizaÃ§Ã£o = Ãrea_peÃ§as / Ãrea_chapa â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Desafio:** 
- PeÃ§as irregulares (nÃ£o-retangulares)
- RotaÃ§Ã£o permitida
- NÃ£o pode haver sobreposiÃ§Ã£o
- Deve caber dentro da chapa

---

## ğŸ§  SoluÃ§Ã£o: Deep Reinforcement Learning

### Passo a Passo

```
1. OBSERVAR
   â†“
2. DECIDIR (CNN + Actor-Critic)
   â†“
3. AGIR (Colocar peÃ§a)
   â†“
4. RECEBER RECOMPENSA
   â†“
5. APRENDER (atualizar rede)
   â†“
   (repetir)
```

---

## ğŸ“¸ RepresentaÃ§Ã£o: Image Encoder

O sistema "vÃª" o layout como uma **imagem de 6 canais**:

```
LAYOUT ATUAL:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     â”Œâ”€â”€â”            â”‚     â†’    CANAL 0: OCUPAÃ‡ÃƒO
â”‚     â”‚â–“â–“â”‚            â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚     â””â”€â”€â”˜  â•±â•²        â”‚          â”‚â–ˆâ–ˆ    â”‚
â”‚          â•±â–“â–“â•²       â”‚          â”‚â–ˆâ–ˆ â–ˆâ–ˆ â”‚
â”‚          â”€â”€â”€â”€        â”‚          â””â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          0=livre, 1=ocupado

                              â†’  CANAL 1: BORDAS
                                 (contornos das peÃ§as)

                              â†’  CANAL 2: DISTÃ‚NCIA
                                 (distÃ¢ncia atÃ© peÃ§a mais prÃ³xima)

                              â†’  CANAL 3: PRÃ“XIMA PEÃ‡A
                                 (peÃ§a a ser colocada)

                              â†’  CANAL 4: DENSIDADE
                                 (quÃ£o "cheio" estÃ¡ localmente)

                              â†’  CANAL 5: ACESSIBILIDADE
                                 (quÃ£o fÃ¡cil Ã© colocar aqui)
```

**Resultado:** Tensor (6, 256, 256) processado pela CNN

---

## ğŸ—ï¸ Arquitetura da Rede

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OBSERVAÃ‡ÃƒO                           â”‚
â”‚  â€¢ Layout image (6, 256, 256)                            â”‚
â”‚  â€¢ Piece features (10 valores)                           â”‚
â”‚  â€¢ Remaining features (10 valores)                       â”‚
â”‚  â€¢ Stats (5 valores)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CNN ENCODER (ResNet + U-Net)                 â”‚
â”‚                                                           â”‚
â”‚  (6,256,256) â†’ Conv â†’ ResBlock â†’ ResBlock â†’ Pool         â”‚
â”‚                 â†“                                         â”‚
â”‚              Embedding (256-dim)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CONCATENATE ALL FEATURES                        â”‚
â”‚  [CNN_emb | piece_feat | remaining | stats]              â”‚
â”‚       256  +    10     +     10    +   5  = 281          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SHARED LAYERS                             â”‚
â”‚  FC(281â†’512) â†’ ReLU â†’ Dropout â†’ FC(512â†’512) â†’ ReLU      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    ACTOR     â”‚              â”‚   CRITIC     â”‚
    â”‚  (PolÃ­tica)  â”‚              â”‚   (Valor)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  POSITION    â”‚              â”‚   VALUE      â”‚
    â”‚   (x, y)     â”‚              â”‚  Estimate    â”‚
    â”‚  [0,1]Ã—[0,1] â”‚              â”‚   Estado     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ROTATION    â”‚
    â”‚   0-35       â”‚
    â”‚ (10Â° bins)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ® Loop de InteraÃ§Ã£o

```
EPISÃ“DIO: Colocar N peÃ§as

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: PeÃ§a 1                                 â”‚
â”‚                                                 â”‚
â”‚  Estado:                                        â”‚
â”‚    â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡  (chapa vazia)                    â”‚
â”‚    PrÃ³xima: â”Œâ”€â”€â”                               â”‚
â”‚             â”‚â–“â–“â”‚                               â”‚
â”‚                                                 â”‚
â”‚  AÃ§Ã£o do agente:                               â”‚
â”‚    position = (0.2, 0.3)  â†’ x=200mm, y=180mm  â”‚
â”‚    rotation = 5           â†’ 50 graus          â”‚
â”‚                                                 â”‚
â”‚  Resultado:                                     â”‚
â”‚    â–¡â”Œâ”€â”€â”â–¡â–¡â–¡â–¡  (peÃ§a colocada!)                 â”‚
â”‚    â–¡â”‚â–“â–“â”‚â–¡â–¡â–¡â–¡                                   â”‚
â”‚                                                 â”‚
â”‚  Recompensa: +1.3 (vÃ¡lida + corner bonus)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: PeÃ§a 2                                 â”‚
â”‚                                                 â”‚
â”‚  Estado:                                        â”‚
â”‚    â–¡â”Œâ”€â”€â”â–¡â–¡â–¡â–¡  (peÃ§a 1 jÃ¡ colocada)            â”‚
â”‚    â–¡â”‚â–“â–“â”‚â–¡â–¡â–¡â–¡                                   â”‚
â”‚    PrÃ³xima: â•±â•²                                 â”‚
â”‚            â•±â–“â–“â•²                                â”‚
â”‚                                                 â”‚
â”‚  AÃ§Ã£o do agente:                               â”‚
â”‚    position = (0.5, 0.4)                       â”‚
â”‚    rotation = 0                                â”‚
â”‚                                                 â”‚
â”‚  Resultado:                                     â”‚
â”‚    â–¡â”Œâ”€â”€â” â•±â•²â–¡  (peÃ§a 2 colocada!)              â”‚
â”‚    â–¡â”‚â–“â–“â”‚â•±â–“â–“â•²                                   â”‚
â”‚                                                 â”‚
â”‚  Recompensa: +1.8 (vÃ¡lida + touching bonus)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... (continua atÃ© todas as peÃ§as) ...

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final: Todas as peÃ§as                          â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â” â•±â•²  â¬¡                                    â”‚
â”‚  â”‚â–“â–“â”‚â•±â–“â–“â•²â¬¡â¬¡                                   â”‚
â”‚  â””â”€â”€â”˜â”€â”€â”€â”€ â¬¡                                    â”‚
â”‚                                                 â”‚
â”‚  UtilizaÃ§Ã£o: 78%                               â”‚
â”‚  Recompensa Total: 95.3                        â”‚
â”‚                                                 â”‚
â”‚  BÃ´nus Final: +78 (78% Ã— 100)                  â”‚
â”‚  TOTAL: 173.3                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Curriculum Learning

O sistema aprende gradualmente, comeÃ§ando fÃ¡cil:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STAGE 1: RetÃ¢ngulos Simples (3-5 peÃ§as)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
  â”Œâ”€â”€â”  â”Œâ”€â”€â”  â”Œâ”€â”€â”
  â””â”€â”€â”˜  â””â”€â”€â”˜  â””â”€â”€â”˜
  
  Objetivo: Aprender conceitos bÃ¡sicos
  - Colocar dentro da chapa
  - Evitar colisÃµes
  - Preferir cantos
  
  Threshold: 60%
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STAGE 2: + RotaÃ§Ã£o (4-7 peÃ§as)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”€â”€â”  â”Œâ”€â”  â•±â”€â”€â•²
  â””â”€â”€â”˜  â””â”€â”˜  â•²â”€â”€â•±
  
  Objetivo: Aprender a rotacionar
  - Usar rotaÃ§Ã£o para encaixar melhor
  - Encontrar orientaÃ§Ã£o Ã³tima
  
  Threshold: 65%
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STAGE 3-4: Formas Regulares (5-15 peÃ§as)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â–¡  â–³  â¬¡  â¬¢
  
  Objetivo: Generalizar para outras formas
  - HexÃ¡gonos, pentÃ¡gonos
  - Mais peÃ§as
  
  Threshold: 65-70%
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STAGE 5-8: Irregulares + Muitas PeÃ§as (10-50)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âˆ¿  âš­  âŸ  âˆ°  âŒ˜  âˆ¾  (muitas formas complexas)
  
  Objetivo: Dominar o problema completo
  - Formas muito irregulares
  - Muitas peÃ§as
  - UtilizaÃ§Ã£o mÃ¡xima
  
  Threshold: 75-80%
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Sistema avanÃ§a automaticamente quando domina o estÃ¡gio atual!**

---

## ğŸ“Š Sistema de Recompensas

```python
reward = 0

# 1. Placement vÃ¡lido
if placement_OK:
    reward += 1.0  âœ…
else:
    reward -= 5.0  âŒ (finaliza episÃ³dio)

# 2. BÃ´nus por tocar outras peÃ§as (economiza espaÃ§o)
if touching_other_pieces:
    reward += 0.5  ğŸ¤

# 3. BÃ´nus por canto (boa estratÃ©gia)
if near_corner:
    reward += 0.3  ğŸ“

# 4. Progresso
reward += 0.1  â¡ï¸

# 5. Penalidade de tempo
reward -= 0.01  â±ï¸

# 6. BÃ´nus final (ao terminar episÃ³dio)
if all_pieces_placed:
    utilization = area_pieces / area_container
    reward += utilization Ã— 100  ğŸ†
    # Ex: 80% utilizaÃ§Ã£o â†’ +80 pontos!
```

---

## ğŸ”„ Algoritmo PPO (Proximal Policy Optimization)

```
LOOP DE TREINAMENTO:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. COLETAR EXPERIÃŠNCIAS                 â”‚
â”‚     - Executar 2048 steps no ambiente   â”‚
â”‚     - Armazenar (s, a, r, s')           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. CALCULAR VANTAGENS (GAE)             â”‚
â”‚     - Advantage = Q(s,a) - V(s)         â”‚
â”‚     - "QuÃ£o melhor foi esta aÃ§Ã£o?"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ATUALIZAR REDE (10 Ã©pocas)           â”‚
â”‚                                          â”‚
â”‚     For epoch in 1..10:                 â”‚
â”‚       For batch in dados:               â”‚
â”‚         â€¢ Calcular loss PPO             â”‚
â”‚         â€¢ Backpropagation               â”‚
â”‚         â€¢ Update weights                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. AVALIAR                              â”‚
â”‚     - Testar em episÃ³dios determin.     â”‚
â”‚     - Medir utilizaÃ§Ã£o                  â”‚
â”‚     - Salvar se melhor                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
                (repetir)
```

---

## ğŸ“ˆ EvoluÃ§Ã£o do Treinamento

```
Iteration 0:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Random behavior
Utilization: ~30%
Stage: 1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”€â”€â” â–¡ â–¡ â–¡ â–¡ â–¡ â–¡ â–¡ â–¡ â–¡  (muito desperdÃ­cio)
  â””â”€â”€â”˜ â–¡ â”Œâ”€â”€â” â–¡ â–¡ â–¡ â–¡ â–¡
       â–¡ â””â”€â”€â”˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Iteration 500:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Learning basic placement
Utilization: ~60%
Stage: 2-3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”€â”€â”â”Œâ”€â”€â” â–¡ â–¡ â–¡ â–¡  (melhor, mas gaps)
  â””â”€â”€â”˜â””â”€â”€â”˜ â”Œâ”€â”
         â–¡ â””â”€â”˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Iteration 2000:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Good packing
Utilization: ~75%
Stage: 4-5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”€â”€â”â”Œâ”€â”€â” â¬¡  (compacto!)
  â””â”€â”€â”˜â””â”€â”€â”˜â¬¡â¬¡
    â•±â•² â”€â”€â”€â”€
   â•±â–“â–“â•²
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Iteration 5000+:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Expert-level packing
Utilization: ~85%
Stage: 7-8
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”â•±â•²â¬¡âˆ¿âš­  (muito eficiente!)
  â””â”˜â”€â”€â¬¢âˆ°âŒ˜
  â”Œâ”€â•±â•²â”€â”âˆ¾
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ MÃ©tricas Principais

### Durante Treinamento

```
Iteration 1000:
â”œâ”€ Loss: 0.0234          â† deve diminuir
â”œâ”€ Utilization: 67%      â† deve aumentar
â”œâ”€ Episodes: 15          â† episÃ³dios completados
â”œâ”€ Stage: 3 (7-12 pcs)   â† curriculum
â””â”€ Learning rate: 3e-4

[EVAL] Utilization: 68% Â± 5%  â† performance real
```

### TensorBoard Plots

```
Loss â†“
  â”‚      
  â”‚â•²     
  â”‚ â•²    
  â”‚  â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â†’ iterations

Utilization â†‘
  â”‚      â•±â”€â”€
  â”‚    â•±
  â”‚  â•±
  â”‚â•±
  â””â”€â”€â”€â”€â”€â”€â”€â†’ iterations

Stage â†‘
  â”‚     __
  â”‚   _â•±
  â”‚  â•±
  â”‚_â•±
  â””â”€â”€â”€â”€â”€â”€â”€â†’ iterations
```

---

## ğŸ’¡ Por que funciona?

1. **RepresentaÃ§Ã£o Visual (CNN)**
   - CNN "vÃª" o layout como humanos veem
   - 6 canais capturam informaÃ§Ãµes relevantes
   - Spatial awareness natural

2. **Reinforcement Learning (PPO)**
   - Aprende por tentativa e erro
   - Maximiza recompensa cumulativa
   - Explora vs. Exploita (entropy)

3. **Curriculum Learning**
   - ComeÃ§a fÃ¡cil, fica difÃ­cil
   - Evita "frustraÃ§Ã£o" do agente
   - ConvergÃªncia mais rÃ¡pida e estÃ¡vel

4. **Actor-Critic Architecture**
   - Actor: aprende polÃ­tica (o que fazer)
   - Critic: aprende valor (quÃ£o bom Ã© o estado)
   - Juntos: aprendizado mais eficiente

---

## ğŸš€ Resultado Final

```
ANTES (Random):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”                   â”‚
â”‚  â””â”€â”€â”˜   â•±â•²             â”‚
â”‚                         â”‚
â”‚           â¬¡â¬¡           â”‚
â”‚                         â”‚
â”‚      â”Œâ”€â”€â”              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
UtilizaÃ§Ã£o: ~30% âŒ

DEPOIS (Trained Agent):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â”Œâ”€â”€â”â•±â•²â¬¡â”Œâ”€â”€â”âˆ¿           â”‚
â”‚â””â”€â”€â”˜â”€â”€â¬¢â””â”€â”€â”˜âš­           â”‚
â”‚â”Œâ”€â”âˆ°âŒ˜â”Œâ”€â”€â”âˆ¾             â”‚
â”‚â””â”€â”˜  â””â”€â”€â”˜               â”‚
â”‚â•±â•² â¬¡â”€â”€â”Œâ”€â”€â”             â”‚
â”‚â”€â”€ â¬¢  â””â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
UtilizaÃ§Ã£o: ~85% âœ…

Melhoria: +55 pontos percentuais!
Economia: $$$$ em material
```

---

## ğŸ‰ ConclusÃ£o

O sistema aprende **sozinho** a resolver um problema complexo:

âœ… Sem regras manuais  
âœ… Sem heurÃ­sticas prÃ©-programadas  
âœ… Apenas: ambiente + recompensa + treinamento  

**E alcanÃ§a performance superior a mÃ©todos tradicionais!**

---

**PrÃ³ximo passo:** `python scripts/quick_test.py` ğŸš€